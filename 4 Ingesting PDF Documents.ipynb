{"cells":[{"cell_type":"markdown","source":["# Ingesting PDF Documents"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bd93b038-0557-4066-ab46-e6ed1e757c55"},{"cell_type":"markdown","source":["## Setup Azure API Keys"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0caf4534-f002-452c-b7f4-a99ddfdc2e35"},{"cell_type":"code","source":["from notebookutils.mssparkutils.credentials import getSecret\n","\n","KEYVAULT_ENDPOINT = \"https://rag-demo-east-us-kv.vault.azure.net/\"\n","# Azure AI Search\n","AI_SEARCH_NAME = getSecret(KEYVAULT_ENDPOINT, \"AI-SEARCH-NAME\")\n","AI_SEARCH_API_KEY = getSecret(KEYVAULT_ENDPOINT, \"AI-SEARCH-API-KEY\")\n","AI_SEARCH_INDEX_NAME = \"rag-demo-index\"\n","# Azure AI Services\n","AI_SERVICES_NAME = getSecret(KEYVAULT_ENDPOINT, \"AI-SERVICES-NAME\")\n","AI_SERVICES_API_KEY = getSecret(KEYVAULT_ENDPOINT, \"AI-SERVICES-API-KEY\")\n","AI_SERVICES_LOCATION = \"eastus\"\n","# Azure Open AI - (if F64 SKU is not used)\n","OPEN_AI_NAME = getSecret(KEYVAULT_ENDPOINT, \"OPEN-AI-NAME\")\n","OPEN_AI_API_KEY = getSecret(KEYVAULT_ENDPOINT, \"OPEN-AI-API-KEY\")\n","#OPEN_AI_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-ada-002\" #1536\n","#OPEN_AI_GPT_DEPLOYMENT_NAME = \"gpt-35-turbo-16k\" # deploymentName could be one of {gpt-35-turbo, gpt-35-turbo-16k}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4345724c-8ad5-4246-a1d4-66302b187e3a","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-20T07:11:47.3390023Z","session_start_time":null,"execution_start_time":"2024-04-20T07:12:07.4499531Z","execution_finish_time":"2024-04-20T07:12:11.8884533Z","parent_msg_id":"69ac2e97-6789-459a-a4e5-eb5465aa29b4"},"text/plain":"StatementMeta(, 4345724c-8ad5-4246-a1d4-66302b187e3a, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fbbccaab-b5ae-4531-8c7b-96b90cdfc4c1"},{"cell_type":"markdown","source":["## Github: Do not embed secrets within the code\n","![Github secrets commit error](https://github.com/luisdza/rag-fabric-workbook/raw/main/images/github-commit.png)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"adafbf46-bd1d-4791-b4e7-8cf39b8cbbc6"},{"cell_type":"markdown","source":["## Load and Analyse the Document"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fc810e0c-39a2-4004-9a2d-8a0ec3d26a51"},{"cell_type":"code","source":["import requests\n","import os\n","\n","url = \"https://github.com/luisdza/rag-fabric-workbook/raw/main/docs/Northwind_Standard_Benefits_Details.pdf\"\n","#url = \"https://github.com/luisdza/rag-fabric-workbook/raw/main/docs/Northwind_Health_Plus_Benefits_Details.pdf\"\n","\n","response = requests.get(url)\n","\n","# Specify your path here\n","path = \"/lakehouse/default/Files/\"\n","\n","# Ensure the directory exists\n","os.makedirs(path, exist_ok=True)\n","\n","# Write the content to a file in the specified path\n","filename = url.rsplit(\"/\")[-1]\n","with open(os.path.join(path, filename), \"wb\") as f:\n","    f.write(response.content)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4345724c-8ad5-4246-a1d4-66302b187e3a","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-20T07:11:47.4566327Z","session_start_time":null,"execution_start_time":"2024-04-20T07:12:12.2264387Z","execution_finish_time":"2024-04-20T07:12:13.0406919Z","parent_msg_id":"37cb6bb7-6ddb-4b95-ad9a-2ed142f4c9ff"},"text/plain":"StatementMeta(, 4345724c-8ad5-4246-a1d4-66302b187e3a, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5734c1b6-12ce-404e-aad2-6a494b56e7c0"},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","document_path = f\"Files/{filename}\"\n","\n","df = spark.read.format(\"binaryFile\").load(document_path).select(\"_metadata.file_name\", \"content\").limit(10).cache()\n","\n","display(df)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"4345724c-8ad5-4246-a1d4-66302b187e3a","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-20T07:11:47.5564166Z","session_start_time":null,"execution_start_time":"2024-04-20T07:12:13.3785335Z","execution_finish_time":"2024-04-20T07:12:14.8592389Z","parent_msg_id":"aee463d6-2f76-423e-a956-b533110c9805"},"text/plain":"StatementMeta(, 4345724c-8ad5-4246-a1d4-66302b187e3a, 5, Finished, Available)"},"metadata":{}},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o4359.load.\n: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/a839d788-eec3-446b-bf34-7aa9e3479a54/user/trusted-service-user/Files/Northwind_Standard_Benefits_Details.pdf?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:690)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1046)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:645)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:635)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1231)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:755)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringType\n\u001b[1;32m      4\u001b[0m document_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinaryFile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata.file_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m      8\u001b[0m display(df)\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:300\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4359.load.\n: Operation failed: \"Bad Request\", 400, HEAD, http://onelake.dfs.fabric.microsoft.com/a839d788-eec3-446b-bf34-7aa9e3479a54/user/trusted-service-user/Files/Northwind_Standard_Benefits_Details.pdf?upn=false&action=getStatus&timeout=90\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:231)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:191)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:189)\n\tat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:690)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1046)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:645)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:635)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.exists(AzureBlobFileSystem.java:1231)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:757)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:755)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:393)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"advisor":{"adviceMetadata":"{\"artifactId\":\"4f1ec4b5-3a31-484d-a486-2f04a7a7207b\",\"activityId\":\"4345724c-8ad5-4246-a1d4-66302b187e3a\",\"applicationId\":\"application_1713597051806_0001\",\"jobGroupId\":\"5\",\"advices\":{\"error\":1}}"}},"id":"80b3c127-d1c3-47f1-9231-3b650a66808e"},{"cell_type":"code","source":["from synapse.ml.services import AnalyzeDocument\n","from pyspark.sql.functions import col\n","\n","analyze_document = (\n","    AnalyzeDocument()\n","    .setPrebuiltModelId(\"prebuilt-layout\")\n","    .setCustomServiceName(AI_SERVICES_NAME)\n","    .setSubscriptionKey(AI_SERVICES_API_KEY)\n","    .setLocation(AI_SERVICES_LOCATION)\n","    .setImageBytesCol(\"content\")\n","    .setOutputCol(\"result\")\n",")\n","\n","analyzed_df = (\n","    analyze_document.transform(df)\n","    .withColumn(\"output_content\", col(\"result.analyzeResult.content\"))\n","    .withColumn(\"paragraphs\", col(\"result.analyzeResult.paragraphs\"))\n",").cache()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2024-04-20T07:11:47.7256535Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-04-20T07:12:14.911512Z","parent_msg_id":"7c82188b-f345-47d2-ab55-bfb703bc2278"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"9acd6068-837c-4da1-b1b0-2c03bd210d1b"},{"cell_type":"code","source":["analyzed_df = analyzed_df.drop(\"content\")\n","display(analyzed_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2024-04-20T07:11:47.8234307Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-04-20T07:12:14.9118659Z","parent_msg_id":"bdce4704-7b85-4ab9-84c0-be5aa628eb89"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"bf792889-7ac1-41c7-83ac-3f736827cd3e"},{"cell_type":"markdown","source":["## Chunking Text"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"664418dd-ecb2-48f5-8ea5-059b907c2685"},{"cell_type":"code","source":["from synapse.ml.featurize.text import PageSplitter\n","\n","ps = (\n","    PageSplitter()\n","    .setInputCol(\"output_content\")\n","    .setMaximumPageLength(4000)\n","    .setMinimumPageLength(3000)\n","    .setOutputCol(\"chunks\")\n",")\n","\n","splitted_df = ps.transform(analyzed_df)\n","display(splitted_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2024-04-20T07:11:47.9902633Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-04-20T07:12:14.9121707Z","parent_msg_id":"d4f71a3a-6012-4d20-b534-f4f81558ad31"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"9eb34e6e-693c-415f-a300-f7324e47e08d"},{"cell_type":"code","source":["from pyspark.sql.functions import posexplode, col, concat\n","\n","# Each \"chunks\" column contains the chunks for a single document in an array\n","# The posexplode function will separate each chunk into its own row\n","exploded_df = splitted_df.select(\"file_name\", posexplode(col(\"chunks\")).alias(\"chunk_index\", \"chunk\"))\n","\n","# Add a unique identifier for each chunk\n","exploded_df = exploded_df.withColumn(\"unique_id\", concat(exploded_df.file_name, exploded_df.chunk_index))\n","\n","# Write the exploded_df DataFrame to a Lakehouse table in Microsoft Fabric\n","exploded_df.write \\\n","    .format(\"delta\") \\\n","    .mode(\"overwrite\") \\\n","    .saveAsTable(\"rag_demo.document_chunks\")\n","\n","display(exploded_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2024-04-20T07:11:48.1009348Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-04-20T07:12:14.9126974Z","parent_msg_id":"be973039-68fd-464b-a218-9c1ba831bc84"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"ea5ea4a1-615d-469b-a320-f339bb7f7210"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"14ebacf4-8ff0-4100-bcde-d086763324b0","known_lakehouses":[{"id":"14ebacf4-8ff0-4100-bcde-d086763324b0"},{"id":"26c6e776-0673-41c3-af3c-efa5234c8a2b"}],"default_lakehouse_name":"rag_demo","default_lakehouse_workspace_id":"8449c164-64ed-4541-ba9e-c9f9a1b997eb"}}},"nbformat":4,"nbformat_minor":5}